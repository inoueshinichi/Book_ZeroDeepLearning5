{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# VAE (Variational Auto Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as TVT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ハイパーパラメータ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784 # 画像データのサイズ(MNIST 28x28=784)\n",
    "hidden_dim = 2000 # NNの中間層の次元サイズ\n",
    "latent_dim = 20 # 潜在変数ベクトルzの次元数\n",
    "epochs = 30\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAD エンコーダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 latent_dim, # 潜在変数zの次元数\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.linear_logvar = nn.Linear(hidden_dim, latent_dim) # 分散varは, logvarとしてエンコーダに出力させる\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear(x)\n",
    "        f = F.relu(h)\n",
    "        mu = self.linear_mu(h)\n",
    "        logvar = self.linear_logvar(h)\n",
    "        sigma = torch.exp(0.5 * logvar)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAD デコーダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 latent_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                z):\n",
    "        h = self.linear1(z)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear2(h)\n",
    "        x_hat = F.sigmoid(h)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数変換トリック\n",
    "```\n",
    "μ ----------------> (+) -----> z\n",
    "                     ↑\n",
    "σ --------> (・)------\n",
    "              ↖\n",
    "N(ε;,0,I)---->ε\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_of_latent_parameters(mu,sigma):\n",
    "    # 標準正規分布 N(0,I)\n",
    "    eps = torch.randn_like(sigma)\n",
    "    z = mu + eps * sigma\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 latent_dim,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = VAE_Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def loss(self, x):\n",
    "        mu, sigma = self.encoder(x)\n",
    "        z = trick_of_latent_parameters(mu, sigma)\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        batch_size = len(x)\n",
    "        L1 = F.mse_loss(x_hat, x, reduction='sum') # MSE 二乗誤差\n",
    "        L2 = - torch.sum(1 + torch.log(sigma**2) - mu**2 - sigma**2) # KL情報量に関する誤差\n",
    "        return (L1 + L2) / batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: C:\\Users\\inoue\\Documents\\MyGithub\\Book_ZeroDeepLearning6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current dir: {str(Path.cwd())}\")\n",
    "\n",
    "# データセット\n",
    "transform = TVT.Compose([\n",
    "    TVT.ToTensor(),\n",
    "    TVT.Lambda(torch.flatten) # 画像をフラットに (28x28->784)\n",
    "])\n",
    "dataset = datasets.MNIST(\n",
    "    root=str(os.sep.join([str(Path.cwd()), 'data'])),\n",
    "    train=True,\n",
    "    download=True, # False, \n",
    "    transform=transform,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルとオプティマイザ\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0.0\n",
    "    cnt = 0\n",
    "    for x, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    loss_avg = loss_sum / cnt\n",
    "    losses.append(loss_avg)\n",
    "    print(loss_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
